---
title: "Project2"
author: "JL"
date: "11/15/2017"
output: html_document
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(jsonlite)
library(tidyverse)
library(tidytext)
library(topicmodels)
library(SnowballC)
library(randomForest)
library(stringi)
library(gridExtra)
library(tidytext)
library(ggthemes)
library(tm)
library(reshape2)
library(text2vec)
library(glmnet)
```

##Abstract
The main objective of this project is to explore and make sense of the massive quantity of data available on amazon reviews.  Are there any patterns present within these reviews that we can use to draw conclusions about the rating of individual reviews? In order to answer this question, we focused on looking for relationships between the score of a single review (from 1-5) and the rest of the variables in our dataset.  Our final deliverable is a model that predicts what the rating of a review will be based on the rest of the components of the review text.  We also created a latent dirichlet model to determine if there were any hidden topics within the data that could help us predict rating.

##Introduction
  How can we predict the rating of an amazon review? Although each review has only a few fields, there is plenty of insight to be gained.  Therefore, in our analysis, we explored both the easily accessible fields in our data, such as review wordcount and number of helpful votes, as well as the latent ones.
	First we looked at the fields which were easily accessible in our dataset, such as review length and number of helpful votes.  However, after an extensive EDA and basic multinomial logistic regression, we weren’t able to produce a model that had sufficient predictive power. Therefore, we turned to the review text in order to predict rating.
  We broke down our text analysis by creating a latent dirichlet model.  We expected our LDA, a common tool for identifying subtopics in texts, to determine the overall topic of the reviews.  For example, if the topic were defined by words such as “poor” or “disappointed,” then the rating would be expected to be low, and vice versa.  After removing common contractions and other unhelpful words, we created our LDA to find those hidden topics within the review text.  However, instead of identifying positive/negative sentiments, our model seemed to simply identify the product categories that we drew our data from.  For example, we were very convinced that the category containing words like “album”, “song”, “sound”, and “track” simply confirmed that we pulled part of our review data from the Digital Music category.  
  Therefore, to better analyze the text in the reviews, we generated a document-term matrix. A document-term matrix describes the frequency of terms in a collection of documents - reviews in this case. We used this information to create a multinomial logistic regression model that predicts the probability of each rating (1,2,3,4,5). This model performed far better than the logistic regression model we used that did not include any information about text review data. 

##Data
Our data contained over 19 million reviews from products in 13 categories across Amazon’s site.  For each review, our data contains a unique reviewer ID key, a unique product ID key, reviewer name, the helpfulness of the review (as voted on by other Amazon users), the text in the review, a summary, the time of the review, and the rating (1-5). The data comes from Amazon reviews from May 1996 to July 2014. The data we used was reduced to 5-core. This means that each of the users and reviews in our dataset have at most 5 reviews each. Due to the nature of our extremely large dataset, we attempted to use Spark and Hadoop to store and process the data.  This document uses a subset of the entire dataset. Cf Project2_HADOOP-temp.Rmd for reference.

```{r,echo=F}
# read in json data if not using hadoop
# combine and label

automotive <- jsonlite::stream_in(
  file("Project Data/reviews_Automotive_5.json"))%>% 
  mutate(name="automotive")

amazon_instant <- jsonlite::stream_in(
  file("Project Data/reviews_Amazon_Instant_Video_5.json"))%>% 
  mutate(name="amazon_instant")

#apps_for_android <- jsonlite::stream_in(
#  file("Project Data/reviews_Apps_for_Android_5.json"))
# 
# baby <- jsonlite::stream_in(
#   file("Project Data/reviews_Baby_5.json"))
# 
# beauty <- jsonlite::stream_in(
#   file("Project Data/reviews_Beauty_5.json"))
# 
# books <- jsonlite::stream_in(
#   file("Project Data/reviews_Books_5.json"))
# 
# cds_vinyl <- jsonlite::stream_in(
#   file("Project Data/reviews_CDs_and_Vinyl_5.json"))
# 
# cell_phones_and_accessories<- jsonlite::stream_in(
#   file("Project Data/reviews_Cell_Phones_and_Accessories_5.json"))
# 
# clothing_shoes <- jsonlite::stream_in(
#   file("Project Data/reviews_Clothing_Shoes_and_Jewelry_5.json"))

digital_music <- jsonlite::stream_in(
  file("Project Data/reviews_Digital_Music_5.json"))%>% 
  mutate(name="digital_music")

# electronics <- jsonlite::stream_in(
#   file("Project Data/reviews_Electronics_5.json"))
# 
# grocery <- jsonlite::stream_in(file(
#   "Project Data/reviews_Grocery_and_Gourmet_Food_5.json"))
# 
# health_and_personal <- jsonlite::stream_in(
#   file("Project Data/reviews_Health_and_Personal_Care_5.json"))
# 
# home_kitchen <- jsonlite::stream_in(file(
#   "Project Data/reviews_Home_and_Kitchen_5.json"))
# 
# kindle_store <- jsonlite::stream_in(file(
#   "Project Data/reviews_Kindle_Store_5.json"))
# 
# movies_and_tv <- jsonlite::stream_in(file(
#   "Project Data/reviews_Movies_and_TV_5.json"))
# 
 musical <- jsonlite::stream_in(file(
   "Project Data/reviews_Musical_Instruments_5.json")) %>% 
   mutate(name="musical")

office_products <- jsonlite::stream_in(
  file("Project Data/reviews_Office_Products_5.json"))%>% 
  mutate(name="office_products")

patio_lawn <- jsonlite::stream_in(
  file("Project Data/reviews_Patio_Lawn_and_Garden_5.json"))%>% 
  mutate(name="patio_lawn")

pet <- jsonlite::stream_in(
  file("Project Data/reviews_Pet_Supplies_5.json"))%>% 
  mutate(name="pet")

 # sports_outdoors <- jsonlite::stream_in(
 #   file("Project Data/reviews_Sports_and_Outdoors_5.json"))

tools <- jsonlite::stream_in(
  file("Project Data/reviews_Tools_and_Home_Improvement_5.json"))%>% 
  mutate(name="tools")

 # toys_games <- jsonlite::stream_in(
 #   file("Project Data/reviews_Toys_and_Games_5.json"))
 # 
 # video_games<- jsonlite::stream_in(
 #   file("Project Data/reviews_Video_Games_5.json"))

df<-rbind(amazon_instant,automotive,digital_music,
          musical,office_products,patio_lawn,pet,tools)
```

```{r,echo=F}
#remove duplicates, add id, reformat date

deduped <- df[!duplicated(df$reviewText), ]
deduped$reviewTime <- as.POSIXct(
  deduped$reviewTime, format = "%m %d, %Y", tz = "EST")
deduped$reviewText<- as.character(deduped$reviewText)
deduped$id <- 1:nrow(deduped)
deduped$reviewTime <- as.POSIXct(deduped$reviewTime)
```

```{r,echo=F}
# create new basic variables and parse "helpful variable"
# create new sampled dataset

df2 <- deduped%>% 
  mutate(reviewLength=stri_length(reviewText)) %>% 
  rename(Rating=overall) %>% 
  filter(reviewLength<6000) %>% #removing outliers
  mutate(helpful_final = dplyr::combine(helpful)[c(T,F)], 
         #the [c(T,F)] structure selects only the desired section of helpful, which was originally a list
         not_helpful_final = dplyr::combine(helpful)[c(F,T)] - dplyr::combine(helpful)[c(T,F)],
         perc_helpful_final = ifelse(helpful_final==0 , 0, helpful_final / (helpful_final + not_helpful_final)), 
         Rating_Scaled=Rating/5,
         reviewLengthFactor=ifelse(reviewLength<200,"Short", ifelse(reviewLength<600,"Medium","Long"))) %>% 
  select(-helpful) #helpful is now redundant
df2_sampled<-sample_n(df2,13000)
```

Distrubution and Visualization of the Variables

```{r}
# Tier 1: Rating
ggplot(df2_sampled,aes(x=Rating))+
  geom_bar(stat="count",aes(fill=as.factor(Rating))) +
  geom_text(stat='bin',aes(label=..count..),vjust=-1,binwidth=1) +
  ggtitle("Ratings Distribution")+theme(legend.position="none")
```
Here we have a simple breakdown of the frequency of Ratings.  We can see that the distribution of rating is very left skewed, as most of the ratings are 5-stars.
```{r}
# Ratings vs Helpful
ggplot(df2_sampled,aes(x=Rating,y=perc_helpful_final,group=Rating)) +
  geom_boxplot() + 
  scale_y_continuous(limits = c(0.00001,1)) +
  ggtitle("Percent of votes as 'Helpful' vs Rating") +
  ylab("% of Votes as 'Helpful'")
```
Here we have a boxplot of rating vs the ratio of helpful votes to total votes (% helpful).  As the rating goes up, the % of votes that were helpful approaches 100%. We had to limit the range of acceptable y-values in this plot to y>0 because we only wanted to view point that had more than 0 votes.  The majority of votes had 0 helpful votes and 0 non-helpful votes, so we had to remove those.
```{r}
# Ratings vs ReviewLength

ggplot(df2_sampled,aes(x=Rating,fill=reviewLengthFactor)) +
  geom_bar(stat="count") +
  ggtitle("Rating Distribution based on Review Length")
```
Here we have a stacked boxplot of review length that we grouped by factor.  We can see that the distribution of short, medium and long reviews are very even across the different ratings.
```{r}
# Tier 1: ReviewLength

ggplot(df2_sampled,aes(x=reviewLength,fill="red")) +
  geom_density() +
  scale_x_continuous(limits = c(0,1000)) + 
  theme_classic() +
  theme(legend.position="none") +
  ggtitle("Distribution of ReviewLength")  #  Values more than 1000 
                                           #  continued the trend
```
Here we have a density plot of the distribution of review length (in terms of word count).  We had to limit the range of the x-axis to x<1000 in order to be able to observe the spike in frequency around x=200.
```{r}
# ReviewLength vs Helpful

ggplot(filter(df2_sampled,helpful_final<100), 
       aes(x=reviewLength, y=helpful_final)) + 
  geom_point() + 
  stat_smooth() +
  ylab("Helpful Votes") +
  ggtitle("ReviewLength vs # of Helpful Votes")
```
Here we have a scatterplot that explores the correlation between review length and number of helpful votes.  We can see from the stat smooth line that there is a slight positive correlation between the two.


```{r}
# Tier 2: Prolific Reviewers

df2b<-sample_n(df2,100000)  #  Create dataset with prolific reviewers

prolific_users<-df2b %>% 
  group_by(reviewerID) %>% 
  summarise(reviews=n(),
            meanRating=mean(Rating),
            meanLength=mean(reviewLength),
            popularity=sum(helpful_final) - sum(not_helpful_final))

# NumReviews vs MeanRating

ggplot(filter(prolific_users,reviews<30),aes(x=reviews,meanRating)) +
  geom_point() +
  stat_smooth() +
  ggtitle("Mean Rating (per user) vs num Reviews (per user)")

# NumReviews vs MeanLength

ggplot(filter(prolific_users,
              meanLength<8000 & reviews<60),
       aes(x=reviews,meanLength)) +
  geom_point() +
  stat_smooth() +
  ggtitle("Mean Review Length (per user) vs num Reviews (per user)")

# NumReviews vs Helpful

ggplot(filter(prolific_users,
              popularity<1000 & reviews<45),
       aes(x=reviews,popularity)) +
  geom_point() +
  stat_smooth() +
  ggtitle("Mean Review Length (per user) vs Popularity (per user)")

# Tier 2: Prolific Products

prolific_products<-df2b %>% 
  group_by(asin) %>% 
  summarise(reviews=n(),
            meanRating=mean(Rating),
            meanLength=mean(reviewLength),
            avg_popularity=mean(perc_helpful_final))

# NumReviews vs MeanRating
ggplot(filter(prolific_products,reviews<30),
       aes(x=reviews,meanRating)) +
  geom_point() +
  stat_smooth() +
  ggtitle("Mean Rating (per product) vs num Reviews (per product)")

# NumReviews vs MeanLength
ggplot(filter(prolific_products,
              meanLength<8000 & reviews<60),
       aes(x=reviews,meanLength)) +
  geom_point() +
  stat_smooth() +
  ggtitle("Mean Review Length (per product) vs num Reviews (per product)")

# NumReviews vs Helpful

ggplot(filter(prolific_products,
              reviews<45),
       aes(x=reviews,y=avg_popularity)) +
  geom_point() +
  stat_smooth() +
  ggtitle("Mean Review Length (per product) vs Avg Helpfulness (per product)")
```

Text Analysis for Groups
```{r}
# Tokenize review text, create word stems, and remove stop words (such as conjunctions)

temp <- df2_sampled %>%
  unnest_tokens(word, reviewText) %>%
  anti_join(stop_words) %>%
  mutate(word = wordStem(word)) %>%
  count(word, sort = T)

final <- df2_sampled %>%
  unnest_tokens(word, reviewText) %>%
  anti_join(stop_words) %>%
  mutate(word = wordStem(word)) %>%
  left_join(temp, on = "word") %>%
  bind_tf_idf(word, id, n)

# plot top words

final %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word))))%>%
  top_n(20) %>%
  ggplot(aes(word, tf_idf)) +
  geom_col() +
  labs(x = NULL, y = "tf-idf") +
  coord_flip()
```

```{r}
# Perform LDA with k=8
# We hypothesized that the 8 latent factors would correspond 
# with the 8 Amazon categories the data comes from

mat_final <- final%>%
  cast_dtm(id, word, n)

ap_lda <- LDA(mat_final, k = 8, control = list(seed = 1234))

ap_lda
str(ap_lda)
ap_topics <- tidy(ap_lda, matrix = "beta")

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

print("Categories grouped on: Amazon Instant Video, 
      Automotive, Digital Music, Musical Instruments, 
      Office Products, Patio/Lawn, Pets, and Tools")

a<-ap_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()+ggtitle("Text Groups")

a

beta_spread <- ap_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

beta_spread
```

```{r}
# running on sample of original data
# glmnet multinomial logistic regression using glmnet

df_features <- df2_sampled
normalized <- function(x){(x-min(x))/(max(x)-min(x))}
df_features$rating_scale <- normalized(df_features$Rating)

# create train/test split

train<-sample_n(df2_sampled,10000)
test<-anti_join(df2_sampled,train)

# Create vocabulary

prep_fun <- tolower
tok_fun <- word_tokenizer

it_train <- itoken(train$reviewText, 
             preprocessor = prep_fun, 
             tokenizer = tok_fun, 
             ids = train$id, 
             progressbar = FALSE)
vocab <- create_vocabulary(it_train)

# Vectorize vocab, make document-term matrix (dtm)

vectorizer <- vocab_vectorizer(vocab)
t1 = Sys.time()
dtm_train <- create_dtm(it_train, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))

# Run glmnet with 4-fold cross validation

nfolds2 = 4
t1 = Sys.time()
glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['Rating']], 
                              family = 'multinomial', 
                              alpha = 1,
                              type.measure = "mse",
                              nfolds = nfolds2,
                              thresh = 1e-3,
                              maxit = 1e3)

#create dtm for test

it_test = test$reviewText %>% 
  prep_fun %>% 
  tok_fun %>% 
  itoken(ids = test$id, 
         progressbar = FALSE)

dtm_test = create_dtm(it_test, vectorizer)

#Predictions on test set -- first row is ID

preds = predict(glmnet_classifier, dtm_test, type = 'response')
print(preds)

preds2 <- as.data.frame(preds)

test$pred_1 <- preds2$`1.1`
test$pred_2 <- preds2$`2.1`
test$pred_3 <- preds2$`3.1`
test$pred_4 <- preds2$`4.1`
test$pred_5 <- preds2$`5.1`
```

#Results & Diagnostics

	Our model directly addresses our research question: can we predict amazon review ratings accurately based on review text data? It takes the text from the amazon review and metadata and tries to predict the review rating (1-5). The coefficients in our multinomial logistic regression model correspond to terms found in the review. They appear in the document-term matrix generated from the train and test data and their frequencies are used by the multinomial logistic regression model. Because there are so many terms, it is not reasonable for us to interpret all of them in context. However, it is clear that the words in a review and their frequencies are important in determining the rating of a review. 
	Our model performs decently, with a MSE of .65. The multinomial logistic model with no text data performed much worse, with an MSE of >1. We believe that this indicates that there is a clear relationship between the contents of review text and the actual review rating. 
	
##Conclusion

  Our results point towards a relationship between review text data and review rating since our model that includes information about text data performed significantly better than our model that did not. We ended up with an MSE of .65 using our model on out-of-sample data. This result is OK, but not good enough to be of much use. 
  However, there is much more that we could have done to improve our model’s performance. We could have identified common words that don’t have much meaning and removed them, created n-grams of the data, performed transformations on the data, performed some sort of sentiment analysis, combined text and non-text features, and experimented with different types of models. Our model barely scratches the surface of the text mining and feature engineering that is available for the text in the review data. We decided to focus our project on managing the large amount of data that we had in hadoop instead of experimenting with models and feature engineering. 
  Moreover, we only performed our analysis on a small portion of the data that we had, sampling from a subset of review categories. We did not manage to get spark to work efficiently and primarily used our cluster to simply hold the data. Our results should not be used for any serious purposes, as our model is not representative of the entire amazon review dataset and our model does not take into account many important text mining features that could be utilized. 